"""
polaroo_process.py
--------------------

This module defines a helper function to process usage data exported from
Polaroo and compute over‚Äëusage ("extra") charges for electricity and
water.  The function can read a local CSV file or a publicly accessible
remote URL, skip any summary section prior to the actual header (the
header line starts with ``name;``), filter the rows to a list of
specified flats or buildings, convert cost columns to numeric values,
and calculate extra charges based on user‚Äëdefined allowances.

Typical usage involves downloading the Polaroo CSV (either via your
scraper or manually), defining a list of flats/buildings to include,
and then calling :func:`process_usage` with the path to that CSV and the
desired allowances.  The resulting DataFrame can be written to an
Excel file by providing an ``output_path``.

Example::

    from polaroo_process import process_usage

    # List of flats or buildings to include (only the street names are
    # used for filtering).  For example, to include all units at
    # ARIBAU 126 and PADILLA 260 you can use:
    filter_addresses = [
        "ARIBAU 126",
        "PADILLA 260",
        # ...add other street names as needed...
    ]

    df = process_usage(
        usage_path="/path/to/polaro_report.csv",
        allowances={'electricity': 100.0, 'water': 50.0},
        delimiter=';',
        decimal=',',
        addresses=filter_addresses,
        output_path="/path/to/xl.xlsx",
    )

The ``addresses`` parameter is optional; if omitted, all rows from the
CSV will be processed.  When provided, it should be a list of
human‚Äëreadable strings identifying flats or buildings (e.g.
"ARIBAU 126 1*2").  Only the first token (typically the street name)
is used to filter the data.  See the documentation of
``process_usage`` for further details.
"""

from __future__ import annotations

import io
import urllib.request
import unicodedata
import re
from pathlib import Path
from typing import Dict, Iterable, Optional

import pandas as pd

# ---------------------------------------------------------------------------
# User configuration
# ---------------------------------------------------------------------------
#
# The following list defines the exact flats (addresses) to include in the
# over‚Äëusage calculation.  Each entry should correspond to one flat in the
# ``name`` column of the Polaroo CSV.  You can adjust this list as needed
# without having to specify addresses on the command line.  If the
# ``addresses`` argument to :func:`process_usage` is omitted or set to
# ``None``, this list will be used by default.

# Room-based allowance limits
ROOM_LIMITS = {
    1: 50,   # 1 room: ‚Ç¨50
    2: 70,   # 2 room: ‚Ç¨70  
    3: 100,  # 3 room: ‚Ç¨100
    4: 130,  # 4 room: ‚Ç¨130
}

# Special limits for specific addresses
SPECIAL_LIMITS = {
    "Padilla 1¬∫ 3¬™": 150,  # Only Padilla 1¬∫ 3¬™: ‚Ç¨150
}

# Address to room mapping - Updated based on Excel sheet data
ADDRESS_ROOM_MAPPING = {
    # Aribau properties - Updated from Excel data
    "Aribau 1¬∫ 1¬™": 1,
    "Aribau 1¬∫ 2¬™": 1,
    "Aribau 2¬∫ 1¬∫": 1,
    "Aribau 2¬∫ 2¬∫": 3,
    "Aribau 2¬∫ 3¬™": 1,
    "Aribau 126-128 3-1": 3,  # Excel shows empty, keeping current value
    "Aribau 3¬∫ 2¬™": 3,
    "Aribau 4¬∫ 1¬™": 3,
    "Aribau 4¬∫ 1¬™ B": 2,
    "Aribau 4¬∫ 2¬™": 3,
    "Aribau Escalera": 1,
    
    # Bisbe Laguarda properties - Updated from Excel data
    "Bisbe Laguarda 14, Pral-2": 3,
    "Bisbe Laguarda 14, 2-2": 3,
    
    # Blasco de Garay properties - Updated from Excel data
    "Blasco de Garay Pral 1¬™": 3,
    "Blasco de Garay Pral 2¬™": 3,
    "Blasco de Garay 1¬∫ 1¬™": 3,
    "Blasco de Garay 1¬∫-2": 2,
    "Blasco de Garay 2¬∫ 1¬™": 3,
    "Blasco de Garay 2¬∫ 2¬™": 3,
    "Blasco de Garay 3¬∫ 1¬™": 3,
    "Blasco de Garay 3¬∫ 2¬™": 3,
    "Blasco de Garay 4¬∫ 1¬™": 3,
    "Blasco de Garay 5¬∫ 1¬™": 1,
    
    # Comte Borrell properties - Updated from Excel data
    "Comte Borrell Pral 1¬™": 3,
    "Comte Borrell 5¬∫ 1¬™": 1,
    "Comte Borrell 5¬∫ 2¬™": 1,
    
    # Llull 250 properties - Updated from Excel data
    "Llull 250 Pral 3": 2,
    "Llull 250 Pral 5": 2,
    "Llull 250 1-1": 2,
    "Llull 250 1-3": 2,
    "Llull 250 1-4": 2,
    "Llull 250 1-5": 2,
    "Llull 250 1-6": 2,
    "Llull 250 2-3": 2,
    "Llull 250 2-5": 2,
    "Llull 250 2-6": 2,
    "Llull 250 3-2": 2,
    "Llull 250 3-4": 2,
    "Llull 250 3-5": 2,
    "Llull 250 3-6": 2,
    "Llull 250 4-3": 2,
    "Llull 250 4-5": 2,
    "Llull 250 5-1": 2,
    "Llull 250 5-2": 2,
    "Llull 250 5-3": 2,
    "Llull 250 5-4": 2,
    "Llull 250 5-5": 2,
    
    # Padilla properties - Updated from Excel data
    "Padilla 1¬∫ 1¬™": 3,  # Excel shows 3 rooms
    "Padilla 1¬∫ 2¬™": 3,  # Excel shows 3 rooms
    "Padilla 1¬∫ 3¬™": 3,  # Excel shows 3 rooms, but special limit ‚Ç¨150 applies
    "Padilla Entl 1¬™": 1,  # From book1 image
    "Padilla Entl 2¬™": 1,  # From book1 image
    "Padilla Entl 4¬™": 1,  # From book1 image
    "Padilla Pral 2¬™": 1,  # From book1 image
    "Padilla Pral 3¬™": 1,  # From book1 image
    "Padilla 2¬∫ 1¬™": 1,  # From book1 image
    "Padilla 2-2": 1,  # From book1 image
    "Padilla 2¬∫ 3¬™": 1,  # From book1 image
    "Padilla 2-4": 1,  # From book1 image
    "Padilla 3¬∫ 2¬™": 1,  # From book1 image
    "Padilla 3¬∫ 3¬™": 1,  # From book1 image
    "Padilla 4¬∫ 2¬™": 1,  # From book1 image
    "Padilla 4¬∫ 3¬™": 1,  # From book1 image
    "Padilla 5¬∫ 1¬™": 1,  # From book1 image
    "Padilla 5¬∫ 2¬™": 1,  # From book1 image
    
    # Psg Sant Joan properties - From book1 image
    "Psg Sant Joan Entl 1¬™": 1,
    "Psg Sant Joan Entl 2¬™": 1,
    "Psg Sant Joan Pral 1¬™": 1,
    "Psg Sant Joan Pral 2¬™": 1,
    "Psg Sant Joan 1¬∫ 1¬™": 1,
    "Pg Sant Joan 161 2-1": 1,
    "Pg Sant Joan 161 2-2": 1,
    "Psg Sant Joan 3¬∫ 2¬™": 1,
    "Pg Sant Joan 4-1": 1,
    "Psg Sant Joan 4¬∫ 2¬™": 1,
    "Psg Sant Joan 5¬∫ 2¬™": 1,
    
    # Providencia properties - From book1 image
    "Providencia Bajs 2¬™": 2,
    "Providencia Pral 2¬™": 2,
    "Providencia 1¬∫ 1¬™": 1,
    "Providencia 1¬∫ 2¬™": 1,
    "Providencia 2¬∫ 1¬™": 1,
    "Providencia 2¬∫ 2¬™": 1,
    "Providencia 3¬∫ 2¬™": 1,
    "Providencia 4¬∫ 2¬™": 1,
    
    # Sardenya properties - From book1 image
    "Sardenya 1¬∫ 2¬™": 2,
    "Sardenya 3¬∫ 2¬™": 2,
    "Sardenya 4¬∫ 2¬™": 2,
    "Sardenya 5¬∫ 2¬™": 2,
    "Sardenya Pral": 1,
    
    # Torrent Olla properties - From book1 image
    "Torrent Olla Pral 1¬™": 1,
    "Torrent Olla 1¬∫ 1¬™": 1,
    "Torrent Olla 1¬∫ 2¬™": 1,
    "Torrent Olla 2¬∫ 1¬™": 1,
    "Torrent Olla 2¬∫ 2¬™": 1,
    "Torrent Olla 3¬∫ 2¬™": 1,
    "Torrent Olla √Åtico": 1,
    
    # Valencia properties - From book1 image
    "Valencia Pral 1¬™": 1,
    "Valencia 2¬∫ 1¬™": 1,
}

USER_ADDRESSES: list[str] = [
    # Aribau properties (10)
    "Aribau 1¬∫ 1¬™",
    "Aribau 1¬∫ 2¬™",
    "Aribau 2¬∫ 1¬∫",
    "Aribau 2¬∫ 2¬∫",
    "Aribau 2¬∫ 3¬™",
    "Aribau 126-128 3-1",
    "Aribau 3¬∫ 2¬™",
    "Aribau 4¬∫ 1¬™",
    "Aribau 4¬∫ 1¬™ B",
    "Aribau 4¬∫ 2¬™",
    
    # Bisbe Laguarda properties (2)
    "Bisbe Laguarda 14, Pral-2",
    "Bisbe Laguarda 14, 2-2",

    # Blasco de Garay properties (10)
    "Blasco de Garay Pral 1¬™",
    "Blasco de Garay Pral 2¬™",
    "Blasco de Garay 1¬∫ 1¬™",
    "Blasco de Garay 1¬∫-2",
    "Blasco de Garay 2¬∫ 1¬™",
    "Blasco de Garay 2¬∫ 2¬™",
    "Blasco de Garay 3¬∫ 1¬™",
    "Blasco de Garay 3¬∫ 2¬™",
    "Blasco de Garay 4¬∫ 1¬™",
    "Blasco de Garay 5¬∫ 1¬™",

    # Comte Borrell properties (3)
    "Comte Borrell Pral 1¬™",
    "Comte Borrell 5¬∫ 1¬™",
    "Comte Borrell 5¬∫ 2¬™",

    # Llull 250 properties (21)
    "Llull 250 Pral 3",
    "Llull 250 Pral 5",
    "Llull 250 1-1",
    "Llull 250 1-3",
    "Llull 250 1-4",
    "Llull 250 1-5",
    "Llull 250 1-6",
    "Llull 250 2-3",
    "Llull 250 2-5",
    "Llull 250 2-6",
    "Llull 250 3-2",
    "Llull 250 3-4",
    "Llull 250 3-5",
    "Llull 250 3-6",
    "Llull 250 4-3",
    "Llull 250 4-5",
    "Llull 250 5-1",
    "Llull 250 5-2",
    "Llull 250 5-3",
    "Llull 250 5-4",
    "Llull 250 5-5",
    
    # Padilla properties (18)
    "Padilla Entl 1¬™",
    "Padilla Entl 2¬™",
    "Padilla Entl 4¬™",
    "Padilla Pral 2¬™",
    "Padilla Pral 3¬™",
    "Padilla 1¬∫ 1¬™",
    "Padilla 1¬∫ 2¬™",
    "Padilla 1¬∫ 3¬™",
    "Padilla 2¬∫ 1¬™",
    "Padilla 2-2",
    "Padilla 2¬∫ 3¬™",
    "Padilla 2-4",
    "Padilla 3¬∫ 2¬™",
    "Padilla 3¬∫ 3¬™",
    "Padilla 4¬∫ 2¬™",
    "Padilla 4¬∫ 3¬™",
    "Padilla 5¬∫ 1¬™",
    "Padilla 5¬∫ 2¬™",
    
    # Psg Sant Joan properties (11)
    "Psg Sant Joan Entl 1¬™",
    "Psg Sant Joan Entl 2¬™",
    "Psg Sant Joan Pral 1¬™",
    "Psg Sant Joan Pral 2¬™",
    "Psg Sant Joan 1¬∫ 1¬™",
    "Pg Sant Joan 161 2-1",
    "Pg Sant Joan 161 2-2",
    "Psg Sant Joan 3¬∫ 2¬™",
    "Pg Sant Joan 4-1",
    "Psg Sant Joan 4¬∫ 2¬™",
    "Psg Sant Joan 5¬∫ 2¬™",
    
    # Providencia properties (8)
    "Providencia Bajs 2¬™",
    "Providencia Pral 2¬™",
    "Providencia 1¬∫ 1¬™",
    "Providencia 1¬∫ 2¬™",
    "Providencia 2¬∫ 1¬™",
    "Providencia 2¬∫ 2¬™",
    "Providencia 3¬∫ 2¬™",
    "Providencia 4¬∫ 2¬™",
    
    # Sardenya properties (5)
    "Sardenya 1¬∫ 2¬™",
    "Sardenya 3¬∫ 2¬™",
    "Sardenya 4¬∫ 2¬™",
    "Sardenya 5¬∫ 2¬™",
    "Sardenya Pral",
    
    # Torrent Olla properties (7)
    "Torrent Olla Pral 1¬™",
    "Torrent Olla 1¬∫ 1¬™",
    "Torrent Olla 1¬∫ 2¬™",
    "Torrent Olla 2¬∫ 1¬™",
    "Torrent Olla 2¬∫ 2¬™",
    "Torrent Olla 3¬∫ 2¬™",
    "Torrent Olla √Åtico",
    
    # Valencia properties (2)
    "Valencia Pral 1¬™",
    "Valencia 2¬∫ 1¬™",
]

def get_allowance_for_address(address: str) -> float:
    """
    Get the allowance limit for a specific address based on room count or special limits.
    
    Parameters
    ----------
    address : str
        The address to look up
        
    Returns
    -------
    float
        The allowance limit in euros
    """
    # Check for exact special limits first
    if address in SPECIAL_LIMITS:
        return SPECIAL_LIMITS[address]
    
    # Check room-based mapping
    if address in ADDRESS_ROOM_MAPPING:
        room_count = ADDRESS_ROOM_MAPPING[address]
        return ROOM_LIMITS.get(room_count, 50)  # Default to ‚Ç¨50 if room count not found
    
    # Default fallback
    return 50.0


# Synonyms mapping from user building keys to dataset building keys.  Some
# buildings are referred to differently in the dataset (e.g. "Comte Borrell"
# corresponds to user input "Borrell").  This mapping ensures that the
# correct dataset rows are matched when filtering by flat.  You can extend
# this dictionary if additional buildings require custom mapping.
_SYNONYMS: dict[str, list[str]] = {
    'ARIBAU': ['ARIBAU'],
    'BLASCO': ['BLASCOGARAY'],
    'GARAY': ['BLASCOGARAY'],
    'BORRELL': ['COMTEBORRELL'],
    'COMTE': ['COMTEBORRELL'],
    'TORRENT': ['TORRENTOLLA'],
    'OLLA': ['TORRENTOLLA'],
    'PROVIDENCIA': ['PROVIDENCIA'],
    'SARDENYA': ['SARDENYA'],
    'PADILLA': ['PADILLA'],
    'VALENCIA': ['VALENCIA'],
}


# ---------------------------------------------------------------------------
# Name parsing helpers
# ---------------------------------------------------------------------------

def _normalize_tokens(s: str) -> str:
    """Normalize a string by removing accents and non‚Äëalphanumeric characters.

    Parameters
    ----------
    s : str
        Input string.

    Returns
    -------
    str
        Uppercase string containing only letters and digits.
    """
    return ''.join(c for c in unicodedata.normalize('NFKD', s) if c.isalnum()).upper()


def _base_code(code: str) -> str:
    """Return the base floor code without trailing letter (if present).

    Examples
    --------
    >>> _base_code('4-1-A')
    '4-1'
    >>> _base_code('PRAL-1')
    'PRAL-1'
    >>> _base_code('5-2')
    '5-2'
    """
    if not code:
        return ''
    parts = code.split('-')
    if len(parts) >= 2 and parts[-1].isalpha():
        return '-'.join(parts[:-1])
    return code


def _parse_floor_dataset(tokens: list[str]) -> str:
    """Parse the floor code from a sequence of tokens for dataset names.

    The function scans the tokens for patterns indicating the floor and
    apartment number.  It recognises special floor words (PRAL, ENTL,
    BAJO, ATICO) with optional numeric suffix, hyphen‚Äëdelimited patterns
    like ``3-2``, and ordinal patterns like ``4¬∫ 1¬™``.  If multiple
    patterns are present, the last one is taken (to avoid confusion
    with building numbers such as "126-128").  Building numbers (e.g.
    "126") and connector words (DE, DEL, etc.) are ignored.

    Parameters
    ----------
    tokens : list of str
        Tokens from the part of the address suspected to contain floor
        information.

    Returns
    -------
    str
        A standardised floor code such as ``'4-1'``, ``'PRAL-1'``,
        ``'ENTL-2'``, or ``''`` if no floor information is detected.
    """
    floor_code = ''
    i = 0
    n = len(tokens)
    while i < n:
        token_orig = tokens[i]
        t = unicodedata.normalize('NFD', token_orig).upper()
        # Check for special floor words
        matched = False
        for variant, code in [
            ('PRINCIPAL', 'PRAL'),
            ('PRAL', 'PRAL'),
            ('ENTRESUELO', 'ENTL'),
            ('ENTL', 'ENTL'),
            ('BAJO', 'BAJO'),
            ('BJO', 'BAJO'),
            ('ATICO', 'ATICO'),
            ('√ÅTICO', 'ATICO'),
            ('ATIC', 'ATICO'),
        ]:
            if t.startswith(variant):
                rest = t[len(variant):].lstrip('- ')
                apt = None
                if rest:
                    rest_clean = re.sub('[¬∫¬™\\.]', '', rest)
                    if rest_clean:
                        apt = rest_clean
                else:
                    # Check next token for apartment number/letter
                    if i + 1 < n:
                        next_tok = unicodedata.normalize('NFD', tokens[i + 1]).upper()
                        next_clean = re.sub('[¬∫¬™\\.]', '', next_tok)
                        if next_clean and (next_clean.isdigit() or re.fullmatch(r'[A-Z]', next_clean)):
                            apt = next_clean
                            i += 1
                floor_code = f"{code}-{apt}" if apt else code
                matched = True
                break
        if matched:
            i += 1
            continue
        # Handle hyphen patterns like '3-2' or '4-1A'
        t_clean = t.replace('¬∫', '').replace('¬™', '')
        m = re.fullmatch(r'([0-9]{1,2})[-]([0-9A-Za-z]{1,2})', t_clean)
        if m:
            floor, apt = m.groups()
            if len(floor) <= 2 and len(apt) <= 2:
                floor_code = f"{floor}-{apt}"
        else:
            # Ordinal pattern like '4¬∫' optionally followed by apartment number/letter
            m2 = re.fullmatch(r'([0-9]{1,2})[¬∫¬™]?', t)
            if m2:
                floor = m2.group(1)
                apt = None
                if i + 1 < n:
                    next_tok = unicodedata.normalize('NFD', tokens[i + 1]).upper()
                    next_clean = re.sub('[¬∫¬™\\.]', '', next_tok)
                    if next_clean and (next_clean.isdigit() or re.fullmatch(r'[A-Z]', next_clean)):
                        apt = next_clean
                        i += 1
                floor_code = f"{floor}-{apt}" if apt else floor
        i += 1
    return floor_code


def _parse_name_dataset(name: str) -> tuple[str, str]:
    """Extract the building key and floor code from a dataset address.

    Parameters
    ----------
    name : str
        Full address string from the Polaroo CSV.

    Returns
    -------
    (str, str)
        ``(building_key, floor_code)`` where ``building_key`` is a
        normalised alphanumeric string representing the building and
        ``floor_code`` is a standardised code for the floor/apartment.
    """
    norm = unicodedata.normalize('NFD', name)
    tokens = norm.split()
    building_tokens: list[str] = []
    floor_tokens: list[str] = []
    found_floor = False
    for token in tokens:
        t = unicodedata.normalize('NFD', token).upper()
        if not found_floor:
            # Detect start of floor part (digits, hyphen, ordinal, or special words)
            if (re.search(r'[0-9]', t) or '-' in t or '¬∫' in t or '¬™' in t or
                any(t.startswith(w) for w in ['PRAL', 'PRINCIPAL', 'ENTL', 'ENTRESUELO', 'BAJO', 'BJO', 'ATICO', '√ÅTICO', 'ATIC'])):
                found_floor = True
                floor_tokens.append(token)
            else:
                # Skip connectors
                if t not in ['DE', 'DEL', 'DA', 'D', 'Y', 'LA', 'LAS', 'LOS', 'AL', 'EL']:
                    building_tokens.append(token)
        else:
            floor_tokens.append(token)
    building_key = _normalize_tokens(' '.join(building_tokens))
    floor_code = _parse_floor_dataset(floor_tokens)
    return building_key, floor_code


def _parse_floor_user(floor_tokens: list[str]) -> str:
    """Parse a user‚Äëprovided floor code from floor tokens.

    This helper interprets patterns like ``1*2``, ``1-2``, ``3¬∫ 1¬™``,
    ``Principal 1``, ``Entresuelo 2`` or ``Atico`` into a standardised
    code.  Building numbers are ignored.  If no floor information is
    detected, an empty string is returned.

    Parameters
    ----------
    floor_tokens : list of str
        Tokens representing the floor portion of a user address.  These
        tokens are assumed to have been identified by
        ``_parse_name_user``.

    Returns
    -------
    str
        Standard floor code such as ``'3-1'``, ``'PRAL-2'``, ``'ENTL-1'``,
        ``'BAJO'``, ``'ATICO'``, or ``''``.
    """
    if not floor_tokens:
        return ''
    # Replace '*' with '-' to normalise star patterns
    tokens = [t.replace('*', '-') for t in floor_tokens]
    s = ' '.join(tokens)
    s_norm = unicodedata.normalize('NFD', s).upper()
    # Remove accents for pattern matching
    s_basic = ''.join(c for c in unicodedata.normalize('NFKD', s_norm) if not unicodedata.combining(c))
    # Check for special floor words first
    for variant, code in [
        ('PRINCIPAL', 'PRAL'),
        ('PRAL', 'PRAL'),
        ('ENTRESUELO', 'ENTL'),
        ('ENTL', 'ENTL'),
        ('BAJO', 'BAJO'),
        ('BJO', 'BAJO'),
        ('ATICO', 'ATICO'),
        ('ATIC', 'ATICO'),
        ('√ÅTICO', 'ATICO'),
    ]:
        m = re.search(rf'{variant}\s*-?\s*([0-9A-Z]{{1,2}})?', s_basic)
        if m:
            apt = m.group(1)
            return f"{code}-{apt}" if apt else code
    # Hyphen pattern like '3-2' or '3-1A'
    m = re.search(r'([0-9]{1,2})\s*-\s*([0-9A-Z]{1,2})', s_basic)
    if m:
        floor, apt = m.groups()
        return f"{floor}-{apt}"
    # Ordinal or plain numeric pattern: extract first two alphanumeric tokens
    s_clean = s_basic.replace('¬∫', '').replace('¬™', '')
    parts = re.findall(r'([0-9]{1,2}|[A-Z])', s_clean)
    if parts:
        floor = parts[0]
        apt = parts[1] if len(parts) > 1 else None
        return f"{floor}-{apt}" if apt else floor
    return ''


def _parse_name_user(addr: str) -> tuple[str, str]:
    """Parse a user address into building key and floor code.

    Building numbers (e.g. ``126``, ``147``) are ignored.  The floor
    portion starts at the first token containing a digit, an ordinal
    symbol, a hyphen or star, or one of the special floor words.  All
    tokens prior to that are considered part of the building name.

    Parameters
    ----------
    addr : str
        Human‚Äëreadable flat identifier provided by the user.

    Returns
    -------
    (str, str)
        Normalised ``(building_key, floor_code)``.
    """
    norm = unicodedata.normalize('NFD', addr)
    tokens = norm.split()
    building_tokens: list[str] = []
    floor_tokens: list[str] = []
    found_floor = False
    for token in tokens:
        t_upper = unicodedata.normalize('NFD', token).upper()
        if not found_floor:
            # Identify the beginning of the floor portion
            if ('*' in t_upper or '-' in t_upper or '¬∫' in t_upper or '¬™' in t_upper or
                any(t_upper.startswith(w) for w in ['PRAL', 'PRINCIPAL', 'ENTRESUELO', 'ENTL', 'BAJO', 'BJO', 'ATICO', '√ÅTICO', 'ATIC']) or
                t_upper.isdigit()):
                # If token is purely digits, assume it's a building number and skip it
                if t_upper.isdigit() and not any(c in t_upper for c in ['*', '-', '¬∫', '¬™']):
                    # skip building number
                    continue
                found_floor = True
                floor_tokens.append(token)
            else:
                if t_upper not in ['DE', 'DEL', 'DA', 'D', 'Y', 'LA', 'LAS', 'LOS', 'AL', 'EL']:
                    building_tokens.append(token)
        else:
            floor_tokens.append(token)
    building_key = _normalize_tokens(' '.join(building_tokens))
    floor_code = _parse_floor_user(floor_tokens)
    return building_key, floor_code



def _sanitize(text: str) -> str:
    """Normalize a string by removing accents and converting to uppercase."""
    nfkd = unicodedata.normalize('NFKD', text)
    no_acc = ''.join(c for c in nfkd if not unicodedata.combining(c))
    return no_acc.upper()


def _read_polaro_file(path: str | Path, *, delimiter: str, decimal: str) -> pd.DataFrame:
    """Load a Polaroo usage file (CSV or Excel), skipping any preamble before the header.

    Polaroo exports typically include a few summary lines before the
    actual data header.  This helper reads the file (either from a
    local path or URL), identifies the header row, and then parses
    the remainder with the appropriate method.

    Parameters
    ----------
    path : str or Path
        Local filesystem path or URL pointing to the file.  URLs
        must be publicly accessible (no authorization headers).
    delimiter : str
        The column delimiter used in CSV files (e.g. ';' for Polaroo exports).
    decimal : str
        The decimal separator used in numeric fields (e.g. ',' for Polaroo exports).

    Returns
    -------
    pandas.DataFrame
        A DataFrame containing all columns from the file.  Numeric
        columns are not converted at this stage.
    """
    path_str = str(path)
    
    # Check if it's an Excel file
    if path_str.lower().endswith(('.xlsx', '.xls')):
        # Handle Excel files
        if path_str.startswith(('http://', 'https://')):
            # Download Excel file from URL
            with urllib.request.urlopen(path_str) as resp:
                excel_data = resp.read()
            df = pd.read_excel(io.BytesIO(excel_data), engine='openpyxl')
        else:
            # Read local Excel file
            df = pd.read_excel(path, engine='openpyxl')
        
        # Simple detection: scan column A for 'name' and use that row as header
        header_index = None
        
        print(f"üîç [EXCEL] Scanning column A for 'name' in {len(df)} rows...")
        
        for idx, row in df.iterrows():
            # Check if column A (first column) contains 'name'
            first_cell = str(row.iloc[0]).lower().strip() if len(row) > 0 and pd.notna(row.iloc[0]) else ""
            if first_cell == 'name':
                header_index = idx
                print(f"‚úÖ [EXCEL] Found 'name' in column A at row {idx}")
                break
        
        if header_index is None:
            print(f"‚ö†Ô∏è [EXCEL] No 'name' found in column A, using default header (row 0)")
            header_index = 0
        
        if header_index is not None and header_index > 0:
            # Manually set header and delete everything before the 'name' row
            print(f"üîç [EXCEL] Found 'name' at row {header_index}, deleting everything before it...")
            
            # Get the header row (the row with 'name')
            header_row = df.iloc[header_index]
            print(f"üîç [EXCEL] Header row values: {list(header_row)}")
            
            # Clean and set column names from the header row
            clean_headers = []
            for i, val in enumerate(header_row):
                if pd.isna(val) or str(val).strip() == '':
                    clean_headers.append(f'Column_{i}')
                else:
                    clean_headers.append(str(val).strip())
            
            print(f"üîç [EXCEL] Clean headers: {clean_headers}")
            df.columns = clean_headers
            
            # Delete everything before and including the header row
            df = df.iloc[header_index + 1:].reset_index(drop=True)
            
            print(f"üîç [EXCEL] After transformation, columns: {list(df.columns)}")
            print(f"üîç [EXCEL] First few rows after transformation:")
            print(df.head().to_string())
        
        return df
    
    else:
        # Handle CSV files (original logic)
        if path_str.startswith(('http://', 'https://')):
            with urllib.request.urlopen(path_str) as resp:
                raw = resp.read().decode('utf-8')
        else:
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                raw = f.read()

        lines = raw.splitlines()
        header_index: Optional[int] = None
        for idx, line in enumerate(lines):
            if line.lower().startswith('name;'):
                header_index = idx
                break
        if header_index is None:
            raise ValueError("Unable to locate the header row starting with 'name;' in the provided CSV.")
        csv_content = '\n'.join(lines[header_index:])
        # Parse using pandas
        df = pd.read_csv(io.StringIO(csv_content), sep=delimiter, decimal=decimal, on_bad_lines='skip')
        return df


def process_usage(
    usage_path: str | Path,
    *,
    output_path: Optional[str | Path] = None,
    allowances: Optional[Dict[str, float]] = None,
    delimiter: str = ';',
    decimal: str = ',',
    addresses: Optional[Iterable[str]] = None,
) -> pd.DataFrame:
    """Read and process a Polaroo usage CSV, returning over‚Äëusage details.

    This function reads the Polaroo CSV from a local file or a public URL,
    extracts the building and floor information from each row in order to
    match against a predefined list of flats, computes over‚Äëusage costs
    based on user allowances, and returns a tidy DataFrame ready for
    export.  If ``addresses`` is not provided, the predefined
    :data:`USER_ADDRESSES` list is used.

    Parameters
    ----------
    usage_path : str or Path
        Path or URL to the Polaroo usage CSV file.  For remote URLs
        the file must be publicly accessible without authentication.
    output_path : str or Path, optional
        If provided, the processed DataFrame will be written to this
        Excel file.  Parent directories are created if necessary.
    allowances : dict, optional
        Mapping of service names to allowed cost thresholds.  Keys
        ``'electricity'`` and ``'water'`` are expected; missing keys
        default to zero.  If ``allowances`` is None or empty, no
        allowances are applied and the extra equals the cost.
    delimiter : str, optional
        Column delimiter used in the CSV.  Polaroo files typically use
        ``';'``.
    decimal : str, optional
        Character used to denote the decimal part in numeric fields.
        Polaroo files typically use ``,``, which will be converted
        appropriately.
    addresses : iterable of str, optional
        List of flat identifiers to include.  If ``None``, the
        predefined :data:`USER_ADDRESSES` is used.  Each address
        should correspond to a flat in the dataset; the parsing logic
        extracts building and floor codes for matching.

    Returns
    -------
    pandas.DataFrame
        A DataFrame with the following columns:

        - ``unit``: full address from the CSV (renamed from ``name``)
        - ``water_provider``: water provider (string or NaN)
        - ``electricity_provider``: electricity provider (string or NaN)
        - ``elec_code``: electricity contract code (string or NaN)
        - ``water_code``: water contract code (string or NaN)
        - ``service_owner``: service owner (electricity or water) if available
        - ``electricity_cost``: electricity cost (float)
        - ``water_cost``: water cost (float)
        - ``elec_extra``: over‚Äëusage electricity cost (float)
        - ``water_extra``: over‚Äëusage water cost (float)
    """
    # Determine addresses to filter by
    if not addresses:
        flat_addresses = USER_ADDRESSES
    else:
        flat_addresses = list(addresses)

    # Read the file (CSV or Excel) into a DataFrame
    usage_df = _read_polaro_file(usage_path, delimiter=delimiter, decimal=decimal)

    # Parse each dataset name into building key and floor code
    bkeys: list[str] = []
    fcodes: list[str] = []
    base_codes: list[str] = []
    letters: list[Optional[str]] = []
    # Debug: Check what columns are available
    print(f"üîç [DEBUG] Available columns: {list(usage_df.columns)}")
    print(f"üîç [DEBUG] Looking for 'name' column...")
    
    # Try different possible column names for the name column
    name_column = None
    for col_name in ['name', 'Name', 'NAME', 'unit', 'Unit', 'UNIT']:
        if col_name in usage_df.columns:
            name_column = col_name
            print(f"‚úÖ [DEBUG] Found name column: '{col_name}'")
            break
    
    if name_column is None:
        print(f"‚ùå [DEBUG] No name column found! Available columns: {list(usage_df.columns)}")
        raise ValueError(f"No 'name' column found in DataFrame. Available columns: {list(usage_df.columns)}")
    
    # Debug: Show what's actually in the name column
    print(f"üîç [DEBUG] Name column '{name_column}' contains {len(usage_df)} values")
    print(f"üîç [DEBUG] First 10 values in name column: {list(usage_df[name_column].head(10))}")
    
    for name in usage_df[name_column].astype(str):
        bkey, fcode = _parse_name_dataset(name)
        bkeys.append(bkey)
        fcodes.append(fcode)
        base = _base_code(fcode)
        base_codes.append(base)
        # extract letter if present
        letter = None
        parts = fcode.split('-') if fcode else []
        if len(parts) >= 2 and parts[-1].isalpha():
            letter = parts[-1]
        letters.append(letter)
    usage_df['_building_key'] = bkeys
    usage_df['_floor_code'] = fcodes
    usage_df['_base_code'] = base_codes
    usage_df['_letter'] = letters

    # Process ALL properties from the Excel file (no filtering)
    print(f"üîç [PROCESSING] Processing ALL {len(usage_df)} properties from Excel file...")
    
    # Use all data instead of filtering
    filtered_df = usage_df.copy()

    # Ensure cost columns are numeric
    for cost_col in ['electricityCost', 'waterCost']:
        if cost_col in filtered_df.columns:
            print(f"üîç [DEBUG] Converting {cost_col} to numeric...")
            print(f"üîç [DEBUG] First 5 values before conversion: {list(filtered_df[cost_col].head())}")
            filtered_df[cost_col] = pd.to_numeric(filtered_df[cost_col], errors='coerce').fillna(0.0)
            print(f"üîç [DEBUG] First 5 values after conversion: {list(filtered_df[cost_col].head())}")
        else:
            print(f"‚ö†Ô∏è [DEBUG] Column {cost_col} not found, setting to 0.0")
            filtered_df[cost_col] = 0.0

    # Select the first available service owner (prefer electricity, then water)
    if 'electricityServiceOwner' in filtered_df.columns or 'waterServiceOwner' in filtered_df.columns:
        so_elec = filtered_df.get('electricityServiceOwner', pd.Series([pd.NA] * len(filtered_df)))
        so_water = filtered_df.get('waterServiceOwner', pd.Series([pd.NA] * len(filtered_df)))
        filtered_df['service_owner'] = so_elec.fillna(so_water)
    else:
        filtered_df['service_owner'] = pd.NA

    # Compute extra charges using room-based allowances
    allowances_list = []
    total_costs = []
    total_extras = []
    elec_extras = []
    water_extras = []
    
    for _, row in filtered_df.iterrows():
        unit_name = row[name_column]  # Use the detected name column
        
        # Get allowance for this specific address
        allowance = get_allowance_for_address(unit_name)
        allowances_list.append(allowance)
        
        # Calculate costs
        elec_cost = row.get('electricityCost', 0.0)
        water_cost = row.get('waterCost', 0.0)
        
        total_cost = elec_cost + water_cost
        
        # Calculate total extra only (when combined cost exceeds allowance)
        total_extra = max(0.0, total_cost - allowance)
        
        total_costs.append(total_cost)
        total_extras.append(total_extra)
        elec_extras.append(0.0)  # No individual elec extra
        water_extras.append(0.0)  # No individual water extra
    
    filtered_df['allowance'] = allowances_list
    filtered_df['total_cost'] = total_costs
    filtered_df['total_extra'] = total_extras
    filtered_df['elec_extra'] = elec_extras
    filtered_df['water_extra'] = water_extras

    # Rename columns to user‚Äëfriendly names
    rename_map = {
        name_column: 'Property',  # Use the detected name column
        'waterProvider': 'water_provider',
        'electricityProvider': 'electricity_provider',
        'electricityCode': 'elec_code',
        'waterCode': 'water_code',
        'electricityCost': 'Electricity Cost',
        'waterCost': 'Water Cost',
    }
    for orig, new in rename_map.items():
        if orig in filtered_df.columns:
            filtered_df.rename(columns={orig: new}, inplace=True)
        else:
            filtered_df[new] = pd.NA

    # Rename allowance column
    filtered_df.rename(columns={'allowance': 'Allowance'}, inplace=True)
    filtered_df.rename(columns={'total_cost': 'Total Cost'}, inplace=True)
    filtered_df.rename(columns={'total_extra': 'Total Extra'}, inplace=True)

    # Debug: Show what's in the DataFrame before final selection
    print(f"üîç [DEBUG] DataFrame before final selection has {len(filtered_df)} rows")
    print(f"üîç [DEBUG] Available columns: {list(filtered_df.columns)}")
    print(f"üîç [DEBUG] First 3 rows of key columns:")
    key_cols = ['Property', 'Allowance', 'Electricity Cost', 'Water Cost', 'Total Cost', 'Total Extra', 'elec_extra', 'water_extra']
    for col in key_cols:
        if col in filtered_df.columns:
            print(f"  {col}: {list(filtered_df[col].head(3))}")
        else:
            print(f"  {col}: COLUMN NOT FOUND")
    
    # Debug: Show the actual Property column values
    if 'Property' in filtered_df.columns:
        print(f"üîç [DEBUG] Property column values (first 5): {list(filtered_df['Property'].head(5))}")
    else:
        print(f"‚ùå [DEBUG] Property column not found!")
    
    # Select final columns in the exact order requested
    final_columns = [
        'Property',
        'Allowance', 
        'Electricity Cost',
        'Water Cost',
        'Total Cost',
        'Total Extra',
        'elec_extra',
        'water_extra',
    ]
    final_df = filtered_df[final_columns].copy()
    
    # Sort alphabetically by Property name
    final_df = final_df.sort_values('Property').reset_index(drop=True)
    
    print(f"üîç [DEBUG] Final DataFrame has {len(final_df)} rows")
    print(f"üîç [DEBUG] Final DataFrame first 3 rows:")
    print(final_df.head(3).to_string())

    # Write to Excel if requested
    if output_path:
        out_path = Path(output_path)
        out_path.parent.mkdir(parents=True, exist_ok=True)
        final_df.to_excel(out_path, index=False)

    return final_df


if __name__ == '__main__':  # pragma: no cover
    import argparse

    parser = argparse.ArgumentParser(description='Process Polaroo usage data to compute over‚Äëusage charges.')
    parser.add_argument('usage_path', help='Local path or public URL to the usage CSV file.')
    parser.add_argument('-o', '--output', dest='output_path', help='Path to save the processed Excel file.')
    parser.add_argument('--elec-allowance', type=float, default=0.0, help='Allowed electricity cost before extra charges are incurred.')
    parser.add_argument('--water-allowance', type=float, default=0.0, help='Allowed water cost before extra charges are incurred.')
    parser.add_argument('--delimiter', type=str, default=';', help="Delimiter used in the CSV file (default ';').")
    parser.add_argument('--decimal', type=str, default=',', help="Decimal separator used in the CSV file (default ',').")
    parser.add_argument(
        '--addresses',
        nargs='*',
        help=(
            'Optional list of flat identifiers to filter by.  If omitted, '
            'the built‚Äëin USER_ADDRESSES list is used.  Each address should '
            'correspond to a specific flat (e.g. "ARIBAU 126 1*2").'
        ),
    )

    args = parser.parse_args()
    allowances = {
        'electricity': args.elec_allowance,
        'water': args.water_allowance,
    }
    df = process_usage(
        usage_path=args.usage_path,
        output_path=args.output_path,
        allowances=allowances,
        delimiter=args.delimiter,
        decimal=args.decimal,
        addresses=args.addresses,
    )
    print(f"Processed {len(df)} records. Preview:")
    print(df.head())